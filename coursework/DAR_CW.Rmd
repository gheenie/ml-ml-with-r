```{r}
renv::install('rmarkdown')
renv::install('knitr')
library(rmarkdown)
library(knitr)
```

---
title: "DAR Coursework"
header-includes:
  - \usepackage{multicol}
  - \usepackage{amsmath}
  - \usepackage{placeins}
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### 1. Statistical learning methods \hfill ($10\%$)\newline

For each of parts (a) through (d), indicate whether we would generally expect the performance of a non-parametric statistical learning method to be better or worse than a parametric method. Justify your answer.

(a) The number of predictors $p$ is large, and the number of observations $n$ is small. ($2\%$)

(b) The sample size $n$ is large, and the number of predictors $p$ is also large. ($2\%$)

(c) The sample size $n$ is small, and the relationship between the predictors and response is highly linear. ($3\%$)

(d) The standard deviation of the error terms, i.e. $\sigma = \textrm{sd}(\varepsilon)$, is extremely high. ($3\%$)

#### 2. Linear regression \hfill ($20\%$)\newline\newline

This question involves the `Auto` dataset included in the “ISLR” package.

(a) Use the `lm()` function to perform a simple linear regression with `acceleration` as the response and `cylinders` as the predictor. Use the `summary()` function to print the results. Comment on the output. For example:

```{r}
renv::install('ISLR')
library(ISLR)

View(Auto)

modelLinearReg <- lm(acceleration ~ cylinders, Auto)
print(summary(modelLinearReg))
```

<!-- -->

i.  Is there a relationship between the predictor and the response? ($3\%$)

There is a linear relationship between cylinders and acceleration. The p-value for the coefficient of cylinders is very small, at lower than 0.1%, which is below our desired p-value. So we reject the null hypothesis that the coefficient of cylinders is 0. The adjusted R-squared is also not 0, so there is a linear relationship.

ii. How strong is the relationship between the predictor and the response? ($3\%$)

The relationship is not very strong, because although the adjusted R-squared is not 0, it is only at 0.2528. This means that the model only explains 25.28% of the variation in acceleration.

iii. Is the relationship between the predictor and the response positive or negative? ($3\%$)

iv. What is the predicted `acceleration` associated with an `cylinders` of 3.0? What are the associated 99% confidence and prediction intervals? ($3\%$)

<!-- -->

(b) Plot the response and the predictor. Use the `abline()` function to display the least squares regression line. ($3\%$)

```{r}
plot(
  acceleration ~ cylinders,
  Auto,
  xlab='acceleration',
  ylab='damage',
  main='Plot of damage against distance'
)
abline(modelLinearReg)

```

(c) Plot the 99% confidence interval and prediction interval in the same plot as (b) using different colours and legends. ($5\%$)

#### 3. Bayesian networks and naïve Bayes classifiers.\hfill ($30\%$)

a)  Given a training dataset including 30 observations and a Bayesian network indicating the relationships between 3 features (i.e. Income, Student and Credit Rate) and the class attribute (i.e. Buy Computer), please create the conditional probability tables by hand. \hfill ($10\%$)

b)  Make predictions for 2 testing observations by using a Bayesian network classifier. \hfill ($5\%$)

c)  Based on the conditional independence assumption between features, please create the conditional probability tables by hand. \hfill ($10\%$)

d)  Make predictions for 2 testing observations by using a naïve Bayes classifier. \hfill ($5\%$)

#### 4. Predicting wine quality by using support vector machine classification algorithm. \hfill ($40\%$)

a)  Download the full wine quality training and testing datasets from Moodle, and use the training dataset to find out the optimal value of hyperparameter C for a linear kernel-based svm. Define the value of the random seed equals 1 and cost = c(0.01, 1, 100). \hfill ($5\%$)

b)  Train a svm classifier by using the linear kernel and the corresponding optimal value of hyperparameter C, then make predictions on the testing dataset, report the classification accuracy. \hfill ($10\%$)

c)  Use the training dataset to find out the optimal values of hyperparameters C and for an RBF kernel-based svm. Define the value of the random seed equals 1, cost = c(0.01, 1, 100) and gamma=c(0.01, 1, 100). \hfill ($5\%$)

d)  Train a svm classifier by using the RBF kernel and the corresponding optimal values of hyperparameters C and gamma, then make predictions on the testing dataset, report the classification accuracy. \hfill ($10\%$)

e)  Train a logistic regression model. Then use the testing dataset to conduct an ROC curve analysis to compare the predictive performance of the trained logistic regression model and those two svm classifiers trained by using linear and RBF kernels respectively. \hfill ($10\%$)
