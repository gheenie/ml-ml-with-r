```{r}
# renv::install('rmarkdown')
# renv::install('knitr')
# renv::install('ISLR')
library(rmarkdown)
library(knitr)
library(ISLR)

```

---
title: "DAR Coursework"
header-includes:
  - \usepackage{multicol}
  - \usepackage{amsmath}
  - \usepackage{placeins}
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### 1. Statistical learning methods \hfill ($10\%$)\newline

For each of parts (a) through (d), indicate whether we would generally expect the performance of a non-parametric statistical learning method to be better or worse than a parametric method. Justify your answer.

(a) The number of predictors $p$ is large, and the number of observations $n$ is small. ($2\%$)

(b) The sample size $n$ is large, and the number of predictors $p$ is also large. ($2\%$)

(c) The sample size $n$ is small, and the relationship between the predictors and response is highly linear. ($3\%$)

(d) The standard deviation of the error terms, i.e. $\sigma = \textrm{sd}(\varepsilon)$, is extremely high. ($3\%$)

#### 2. Linear regression \hfill ($20\%$)\newline\newline

This question involves the `Auto` dataset included in the “ISLR” package.

(a) Use the `lm()` function to perform a simple linear regression with `acceleration` as the response and `cylinders` as the predictor. Use the `summary()` function to print the results. Comment on the output. For example:

```{r}
# Model the least squares line.
modelLinearReg <- lm(acceleration ~ cylinders, Auto)
print(summary(modelLinearReg))

```

<!-- -->

i.  Is there a relationship between the predictor and the response? ($3\%$)

There is a linear relationship between cylinders and acceleration. The p-value for the coefficient of cylinders is very small, at lower than 0.1%, which is below our desired p-value. So we reject the null hypothesis that the coefficient of cylinders is 0. The adjusted R-squared is also not 0, so there is a linear relationship.

ii. How strong is the relationship between the predictor and the response? ($3\%$)

The relationship is not very strong, because although the adjusted R-squared is not 0, it is only at 0.2528. This means that the model only explains 25.28% of the variation in acceleration.

iii. Is the relationship between the predictor and the response positive or negative? ($3\%$)

The relationship is negative because the coefficient of cylinders is negative.

iv. What is the predicted `acceleration` associated with an `cylinders` of 3.0? What are the associated 99% confidence and prediction intervals? ($3\%$)

```{r}
# Prepare data.
uniqueCylinders = sort(unique(Auto[, 'cylinders']), decreasing=FALSE)
print(uniqueCylinders)

# Predict on unique cylinder numbers.
prediction <- predict(
  modelLinearReg,
  newdata=data.frame(cylinders=uniqueCylinders)
)
print(prediction)

confidenceInterval <- predict(
  modelLinearReg,
  newdata=data.frame(cylinders=uniqueCylinders),
  interval='confidence',
  level=.99
)
print(confidenceInterval)

predictionInterval <- predict(
  modelLinearReg,
  newdata=data.frame(cylinders=uniqueCylinders),
  interval='prediction',
  level=.99
)
print(predictionInterval)

```
For 3.0 cylinders, the predicted acceleration is 17.55906, the confidence interval is 17.00962 - 18.10849, and the prediction interval is 11.361636 - 23.75648. The confidence and prediction intervals for the rest of the cylinders are shown above, with the row numbers corresponding to the row numbers in uniqueCylinders representing the number of cylinders.

<!-- -->

(b) Plot the response and the predictor. Use the `abline()` function to display the least squares regression line. ($3\%$)

```{r}
plot(
  acceleration ~ cylinders,
  Auto,
  xlab='acceleration',
  ylab='damage',
  main='Plot of damage against distance'
)
abline(modelLinearReg)

```

(c) Plot the 99% confidence interval and prediction interval in the same plot as (b) using different colours and legends. ($5\%$)

```{r}
plot(
  acceleration ~ cylinders,
  Auto,
  xlab='acceleration',
  ylab='damage',
  main='Plot of damage against distance',
  ylim=c(7, 25)
)
abline(modelLinearReg)

lines(
  confidenceInterval[, 'lwr'] ~ uniqueCylinders,
  col='red',
  pch='+',
  type='b'
)
lines(
  confidenceInterval[, 'upr'] ~ uniqueCylinders,
  col='red',
  pch='+',
  type='b'
)
lines(
  predictionInterval[, 'lwr'] ~ uniqueCylinders,
  col='blue',
  pch='*',
  type='b'
)
lines(
  predictionInterval[, 'upr'] ~ uniqueCylinders,
  col='blue',
  pch='*',
  type='b'
)
legend(
  'topright',
  pch=c('+', '*'),
  col=c('red', 'blue'),
  legend=c('confidence', 'prediction')
)

```


#### 3. Bayesian networks and naïve Bayes classifiers.\hfill ($30\%$)

a)  Given a training dataset including 30 observations and a Bayesian network indicating the relationships between 3 features (i.e. Income, Student and Credit Rate) and the class attribute (i.e. Buy Computer), please create the conditional probability tables by hand. \hfill ($10\%$)

b)  Make predictions for 2 testing observations by using a Bayesian network classifier. \hfill ($5\%$)

c)  Based on the conditional independence assumption between features, please create the conditional probability tables by hand. \hfill ($10\%$)

d)  Make predictions for 2 testing observations by using a naïve Bayes classifier. \hfill ($5\%$)

#### 4. Predicting wine quality by using support vector machine classification algorithm. \hfill ($40\%$)

a)  Download the full wine quality training and testing datasets from Moodle, and use the training dataset to find out the optimal value of hyperparameter C for a linear kernel-based svm. Define the value of the random seed equals 1 and cost = c(0.01, 1, 100). \hfill ($5\%$)

```{r}
dataTrain = read.table(
  'WineQuality_training.txt',
  header=TRUE,
  sep=',',
  dec='.'
)
dataTrain$quality <- as.factor(dataTrain$quality)
dataTest = read.table(
  'WineQuality_testing.txt',
  header=TRUE,
  sep=',',
  dec='.'
)
dataTest$quality <- as.factor(dataTest$quality)

set.seed(1)
tunedSvm <- tune(
  svm,
  quality ~ .,
  data=dataTrain,
  kernel='linear',
  ranges=list(cost=c(0.01, 1, 100))
)
print(summary(tunedSvm))

```
The most optimal value of the cost hyperparameter is 1 as it produces the lowest error. This is for tuning with scale set to TRUE. Tuning the SVM with scale set to FALSE takes too much time, so it is not performed.

b)  Train a svm classifier by using the linear kernel and the corresponding optimal value of hyperparameter C, then make predictions on the testing dataset, report the classification accuracy. \hfill ($10\%$)

```{r}
# Without scaling.
modelSvmLinearNoScale <- svm(
  quality ~ .,
  data=dataTrain,
  kernel='linear',
  cost=1,
  scale=FALSE
)
# print(summary(modelSvmLinear))
# plot(modelSvmLinear, dataTrain, volatile.acidity ~ residual.sugar)
pred <- predict(modelSvmLinear, newdata=dataTest[, -12])
mean(pred == dataTest[, 12])

# With scaling.
modelSvmLinear <- svm(
  quality ~ .,
  data=dataTrain,
  kernel='linear',
  cost=1,
  scale=TRUE
)
# print(summary(modelSvmLinear))
# plot(modelSvmLinear, dataTrain, volatile.acidity ~ residual.sugar)
pred <- predict(modelSvmLinear, newdata=dataTest[, -12])
mean(pred == dataTest[, 12])

```

c)  Use the training dataset to find out the optimal values of hyperparameters C and for an RBF kernel-based svm. Define the value of the random seed equals 1, cost = c(0.01, 1, 100) and gamma=c(0.01, 1, 100). \hfill ($5\%$)

d)  Train a svm classifier by using the RBF kernel and the corresponding optimal values of hyperparameters C and gamma, then make predictions on the testing dataset, report the classification accuracy. \hfill ($10\%$)

e)  Train a logistic regression model. Then use the testing dataset to conduct an ROC curve analysis to compare the predictive performance of the trained logistic regression model and those two svm classifiers trained by using linear and RBF kernels respectively. \hfill ($10\%$)
