```{r}
# renv::install('rmarkdown')
# renv::install('knitr')
# renv::install('ISLR')
# renv::install('aod')
# renv::install('ROCR')
library(rmarkdown)
library(knitr)
library(ISLR)
library(aod)
library(ROCR)
library(e1071)

```

---
title: "DAR Coursework"
header-includes:
  - \usepackage{multicol}
  - \usepackage{amsmath}
  - \usepackage{placeins}
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### 1. Statistical learning methods \hfill ($10\%$)\newline

For each of parts (a) through (d), indicate whether we would generally expect the performance of a non-parametric statistical learning method to be better or worse than a parametric method. Justify your answer.

(a) The number of predictors $p$ is large, and the number of observations $n$ is small. ($2\%$)

(b) The sample size $n$ is large, and the number of predictors $p$ is also large. ($2\%$)

(c) The sample size $n$ is small, and the relationship between the predictors and response is highly linear. ($3\%$)

(d) The standard deviation of the error terms, i.e. $\sigma = \textrm{sd}(\varepsilon)$, is extremely high. ($3\%$)

#### 2. Linear regression \hfill ($20\%$)\newline\newline

This question involves the `Auto` dataset included in the “ISLR” package.

(a) Use the `lm()` function to perform a simple linear regression with `acceleration` as the response and `cylinders` as the predictor. Use the `summary()` function to print the results. Comment on the output. For example:

```{r}
# Model the least squares line.
modelLinearReg <- lm(acceleration ~ cylinders, Auto)
print(summary(modelLinearReg))

```

<!-- -->

i.  Is there a relationship between the predictor and the response? ($3\%$)

There is a linear relationship between cylinders and acceleration. The p-value for the coefficient of cylinders is very small, at lower than 0.1%, which is below our desired p-value. So we reject the null hypothesis that the coefficient of cylinders is 0. The adjusted R-squared is also not 0, so there is a linear relationship.

ii. How strong is the relationship between the predictor and the response? ($3\%$)

The relationship is not very strong, because although the adjusted R-squared is not 0, it is only at 0.2528. This means that the model only explains 25.28% of the variation in acceleration.

iii. Is the relationship between the predictor and the response positive or negative? ($3\%$)

The relationship is negative because the coefficient of cylinders is negative.

iv. What is the predicted `acceleration` associated with an `cylinders` of 3.0? What are the associated 99% confidence and prediction intervals? ($3\%$)

```{r}
# Prepare data.
uniqueCylinders = sort(unique(Auto[, 'cylinders']), decreasing=FALSE)
print(uniqueCylinders)
writeLines('')

# Predict on unique cylinder numbers.
prediction <- predict(
  modelLinearReg,
  newdata=data.frame(cylinders=uniqueCylinders)
)
print(prediction)
writeLines('')

confidenceInterval <- predict(
  modelLinearReg,
  newdata=data.frame(cylinders=uniqueCylinders),
  interval='confidence',
  level=.99
)
print(confidenceInterval)
writeLines('')

predictionInterval <- predict(
  modelLinearReg,
  newdata=data.frame(cylinders=uniqueCylinders),
  interval='prediction',
  level=.99
)
print(predictionInterval)
writeLines('')

```

For 3.0 cylinders, the predicted acceleration is 17.55906, the confidence interval is 17.00962 - 18.10849, and the prediction interval is 11.361636 - 23.75648. The confidence and prediction intervals for the rest of the cylinders are shown above, with the row numbers corresponding to the row numbers in uniqueCylinders representing the number of cylinders.

<!-- -->

(b) Plot the response and the predictor. Use the `abline()` function to display the least squares regression line. ($3\%$)

```{r}
# Plot the dataset.
plot(
  acceleration ~ cylinders,
  Auto,
  xlab='acceleration',
  ylab='damage',
  main='Plot of damage against distance'
)
# Then plot the least squares line.
abline(modelLinearReg)

```

(c) Plot the 99% confidence interval and prediction interval in the same plot as (b) using different colours and legends. ($5\%$)

```{r}
# Re-plot the dataset and least squares line.
plot(
  acceleration ~ cylinders,
  Auto,
  xlab='acceleration',
  ylab='damage',
  main='Plot of damage against distance',
  ylim=c(7, 25)
)
abline(modelLinearReg)

# Plot the lines for confidence and prediction intervals.
lines(
  confidenceInterval[, 'lwr'] ~ uniqueCylinders,
  col='red',
  pch='+',
  type='b'
)
lines(
  confidenceInterval[, 'upr'] ~ uniqueCylinders,
  col='red',
  pch='+',
  type='b'
)
lines(
  predictionInterval[, 'lwr'] ~ uniqueCylinders,
  col='blue',
  pch='*',
  type='b'
)
lines(
  predictionInterval[, 'upr'] ~ uniqueCylinders,
  col='blue',
  pch='*',
  type='b'
)
legend(
  'topright',
  pch=c('+', '*'),
  col=c('red', 'blue'),
  legend=c('confidence', 'prediction')
)

```


#### 3. Bayesian networks and naïve Bayes classifiers.\hfill ($30\%$)

a)  Given a training dataset including 30 observations and a Bayesian network indicating the relationships between 3 features (i.e. Income, Student and Credit Rate) and the class attribute (i.e. Buy Computer), please create the conditional probability tables by hand. \hfill ($10\%$)

```{r}
# Inspect data.
bayesianTrainingData = read.table(
  'question 3.txt',
  header=TRUE,
  sep=' '
)

# Conditional probability table for Credit Rating.
income = c('low', 'low', 'low', 'low', 'high', 'high', 'high', 'high')
student = c('false', 'false', 'true', 'true', 'false', 'false', 'true', 'true')
buyComputer = c('no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'yes')
fairCreditRating = c(0.667, 0.333, 0.75, 0.5, 0.333, 0.333, 0.75, 0.5)
excellentCreditRating = c(0.333, 0.667, 0.25, 0.5, 0.667, 0.667, 0.25, 0.5)
creditRatingCpt = data.frame(
  cbind(income, student, buyComputer, fairCreditRating, excellentCreditRating)
)
colnames(creditRatingCpt) <- c(
  'Income',
  'Student',
  'Buy Computer',
  'Credit Rating = Fair',
  'Credit Rating = Excellent'
)
print.data.frame(creditRatingCpt, row.names=FALSE)
writeLines('')

# Conditional probability table for Income.
buyComputer <- c('no', 'yes')
lowIncome <- c(0.611, 0.583)
highIncome <- c(0.389, 0.417)
incomeCpt <- data.frame(cbind(buyComputer, lowIncome, highIncome))
colnames(incomeCpt) <- c('Buy Computer', 'Income = Low', 'Income = High')
print.data.frame(incomeCpt, row.names=FALSE)
writeLines('')

# Conditional probability table for Student.
falseStudent <- c(0.333, 0.5)
trueStudent <- c(0.667, 0.5)
studentCpt <- data.frame(cbind(buyComputer, falseStudent, trueStudent))
colnames(studentCpt) <- c('Buy Computer', 'Student = False', 'Student = True')
print.data.frame(studentCpt, row.names=FALSE)
writeLines('')

# Conditional probability table for Buy Computer.
noBuyComputer <- c(0.6)
yesBuyComputer <- c(0.4)
buyComputerCpt <- data.frame(cbind(noBuyComputer, yesBuyComputer))
colnames(buyComputerCpt) <- c('Buy Computer = No', 'Buy Computer = Yes')
print.data.frame(buyComputerCpt, row.names=FALSE)
writeLines('')

```

From top to bottom, the code output shows the conditional probability tables for Credit Rating, Income, Student, and Buy Computer respectively.

b)  Make predictions for 2 testing observations by using a Bayesian network classifier. \hfill ($5\%$)

For observation 31:

P(Buy Computer=No | Income=Low, Student=True, Credit Rating=Excellent)
is proportional to P(Income=Low, Student=True, Credit Rating=Excellent, Buy Computer=No)
= P(Income=Low | Buy Computer=No)
* P(Student=True | Buy Computer=No)
* P(Credit Rating=Excellent | Income=Low, Student=True, Buy Computer=No)
* P(Buy Computer=No)
= 0.611 * 0.667 * 0.25 * 0.6
= 0.06113055

P(Buy Computer=Yes | Income=Low, Student=True, Credit Rating=Excellent)
is proportional to P(Income=Low, Student=True, Credit Rating=Excellent, Buy Computer=Yes)
= P(Income=Low | Buy Computer=Yes)
* P(Student=True | Buy Computer=Yes)
* P(Credit Rating=Excellent | Income=Low, Student=True, Buy Computer=Yes)
* P(Buy Computer=Yes)
= 0.583 * 0.5 * 0.5 * 0.4
= 0.0583

Hence, the prediction for observation 31 is No, because P(Buy Computer=No | Income=Low, Student=True, Credit Rating=Excellent) is greater than P(Buy Computer=Yes | Income=Low, Student=True, Credit Rating=Excellent).

For observation 32:

P(Buy Computer=No | Income=High, Student=True, Credit Rating=Fair)
is proportional to P(Income=High, Student=True, Credit Rating=Fair, Buy Computer=No)
= P(Income=High | Buy Computer=No)
* P(Student=True | Buy Computer=No)
* P(Credit Rating=Fair | Income=High, Student=True, Buy Computer=No)
* P(Buy Computer=No)
= 0.389 * 0.667 * 0.75 * 0.6
= 0.11675835

P(Buy Computer=Yes | Income=High, Student=True, Credit Rating=Fair)
is proportional to P(Income=High, Student=True, Credit Rating=Fair, Buy Computer=Yes)
= P(Income=High | Buy Computer=Yes)
* P(Student=True | Buy Computer=Yes)
* P(Credit Rating=Fair | Income=High, Student=True, Buy Computer=Yes)
* P(Buy Computer=Yes)
= 0.417 * 0.5 * 0.5 * 0.4
= 0.0417

Hence, the prediction for observation 32 is No, because P(Buy Computer=No | Income=High, Student=True, Credit Rating=Fair) is greater than P(Buy Computer=Yes | Income=High, Student=True, Credit Rating=Fair).

c)  Based on the conditional independence assumption between features, please create the conditional probability tables by hand. \hfill ($10\%$)

```{r}
# Conditional probability table for Credit Rating.
buyComputer <- c('no', 'yes')
fairCreditRating <-c(0.667, 0.417)
excellentCreditRating <- c(0.333, 0.583)
creditRatingCpt <- data.frame(
  cbind(buyComputer, fairCreditRating, excellentCreditRating)
)
colnames(creditRatingCpt) <- c(
  'Buy Computer',
  'Credit Rating = Fair',
  'Credit Rating = Excellent'
)
print.data.frame(creditRatingCpt, row.names=FALSE)
writeLines('')

# Conditional probability table for Income.
lowIncome <- c(0.611, 0.583)
highIncome <- c(0.389, 0.417)
incomeCpt <- data.frame(cbind(buyComputer, lowIncome, highIncome))
colnames(incomeCpt) <- c('Buy Computer', 'Income = Low', 'Income = High')
print.data.frame(incomeCpt, row.names=FALSE)
writeLines('')

# Conditional probability table for Student.
falseStudent <- c(0.333, 0.5)
trueStudent <- c(0.667, 0.5)
studentCpt <- data.frame(cbind(buyComputer, falseStudent, trueStudent))
colnames(studentCpt) <- c('Buy Computer', 'Student = False', 'Student = True')
print.data.frame(studentCpt, row.names=FALSE)
writeLines('')

# Conditional probability table for Buy Computer.
noBuyComputer <- c(0.6)
yesBuyComputer <- c(0.4)
buyComputerCpt <- data.frame(cbind(noBuyComputer, yesBuyComputer))
colnames(buyComputerCpt) <- c('Buy Computer = No', 'Buy Computer = Yes')
print.data.frame(buyComputerCpt, row.names=FALSE)
writeLines('')

```

From top to bottom, the code output shows the conditional probability tables for Credit Rating, Income, Student, and Buy Computer respectively, based on the conditional independence assumption where the predictors are assumed to be independent of each other. Only the table for Credit Rating has changed from question 3b as it is the only predictor dependent on other predictors.

d)  Make predictions for 2 testing observations by using a naïve Bayes classifier. \hfill ($5\%$)

For observation 31:

P(Buy Computer=No | Income=Low, Student=True, Credit Rating=Excellent)
is proportional to P(Income=Low, Student=True, Credit Rating=Excellent, Buy Computer=No)
= P(Income=Low | Buy Computer=No)
* P(Student=True | Buy Computer=No)
* P(Credit Rating=Excellent | Buy Computer=No)
* P(Buy Computer=No)
= 0.611 * 0.667 * 0.333 * 0.6
= 0.0814258926

P(Buy Computer=Yes | Income=Low, Student=True, Credit Rating=Excellent)
is proportional to P(Income=Low, Student=True, Credit Rating=Excellent, Buy Computer=Yes)
= P(Income=Low | Buy Computer=Yes)
* P(Student=True | Buy Computer=Yes)
* P(Credit Rating=Excellent | Buy Computer=Yes)
* P(Buy Computer=Yes)
= 0.583 * 0.5 * 0.583 * 0.4
= 0.0679778

Hence, the prediction for observation 31 is No, because P(Buy Computer=No | Income=Low, Student=True, Credit Rating=Excellent) is greater than P(Buy Computer=Yes | Income=Low, Student=True, Credit Rating=Excellent).

For observation 32:

P(Buy Computer=No | Income=High, Student=True, Credit Rating=Fair)
is proportional to P(Income=High, Student=True, Credit Rating=Fair, Buy Computer=No)
= P(Income=High | Buy Computer=No)
* P(Student=True | Buy Computer=No)
* P(Credit Rating=Fair | Buy Computer=No)
* P(Buy Computer=No)
= 0.389 * 0.667 * 0.667 * 0.6
= 0.1038370926

P(Buy Computer=Yes | Income=High, Student=True, Credit Rating=Fair)
is proportional to P(Income=High, Student=True, Credit Rating=Fair, Buy Computer=Yes)
= P(Income=High | Buy Computer=Yes)
* P(Student=True | Buy Computer=Yes)
* P(Credit Rating=Fair | Buy Computer=Yes)
* P(Buy Computer=Yes)
= 0.417 * 0.5 * 0.417 * 0.4
= 0.0347778

Hence, the prediction for observation 32 is No, because P(Buy Computer=No | Income=High, Student=True, Credit Rating=Fair) is greater than P(Buy Computer=Yes | Income=High, Student=True, Credit Rating=Fair).

#### 4. Predicting wine quality by using support vector machine classification algorithm. \hfill ($40\%$)

a)  Download the full wine quality training and testing datasets from Moodle, and use the training dataset to find out the optimal value of hyperparameter C for a linear kernel-based svm. Define the value of the random seed equals 1 and cost = c(0.01, 1, 100). \hfill ($5\%$)

```{r}
# Prepare data.
dataTrain = read.table(
  'WineQuality_training.txt',
  header=TRUE,
  sep=',',
  dec='.'
)
dataTrain$quality <- as.factor(dataTrain$quality)
dataTest = read.table(
  'WineQuality_testing.txt',
  header=TRUE,
  sep=',',
  dec='.'
)
dataTest$quality <- as.factor(dataTest$quality)

# Comparing different costs with 10-fold cross-validation.
set.seed(1)
tunedSvmLinear <- tune(
  svm,
  quality ~ .,
  data=dataTrain,
  kernel='linear',
  ranges=list(cost=c(0.01, 1, 100))
)
print(summary(tunedSvmLinear))

```

The most optimal value of the cost hyperparameter is 1 as it produced the lowest error of 0.238. This is for tuning with scale set to TRUE. Tuning the SVM with scale set to FALSE takes too much time, so it is not performed.

b)  Train a svm classifier by using the linear kernel and the corresponding optimal value of hyperparameter C, then make predictions on the testing dataset, report the classification accuracy. \hfill ($10\%$)

```{r}
# With scaling.
modelSvmLinear <- svm(
  quality ~ .,
  data=dataTrain,
  kernel='linear',
  cost=1,
  scale=TRUE
)
pred <- predict(modelSvmLinear, newdata=dataTest[, -12])
mean(pred == dataTest[, 12])

# Without scaling.
modelSvmLinear <- svm(
  quality ~ .,
  data=dataTrain,
  kernel='linear',
  cost=1,
  scale=FALSE
)
pred <- predict(modelSvmLinear, newdata=dataTest[, -12])
mean(pred == dataTest[, 12])

```

The optimal value of the cost hyperparameter is set to 1. The classification accuracy is 68.25% when trained with scaling, and 69.75% when trained without scaling. Training without scaling resulted in a slightly higher testing accuracy, although SVM training should usually be performed with scaling, it took a noticeably longer time, and there could be a better cost hyperparameter as tuning was only done with scaling.

c)  Use the training dataset to find out the optimal values of hyperparameters C and for an RBF kernel-based svm. Define the value of the random seed equals 1, cost = c(0.01, 1, 100) and gamma=c(0.01, 1, 100). \hfill ($5\%$)

```{r}
# Compare different costs and gammas with 10-fold cross-validation.
set.seed(1)
tunedSvmRadial <- tune(
  svm,
  quality ~ .,
  data=dataTrain,
  kernel='radial',
  ranges=list(
    cost=c(0.01, 1, 100),
    gamma=c(0.01, 1, 100)
  )
)
print(summary(tunedSvmRadial))

```

The most optimal values of the cost and gamma hyperparameters are 100 and 1 respectively as it produced the lowest error of 0.1556667. This is for tuning with scale set to TRUE. Tuning the SVM with scale set to FALSE takes too much time, so it is not performed.

d)  Train a svm classifier by using the RBF kernel and the corresponding optimal values of hyperparameters C and gamma, then make predictions on the testing dataset, report the classification accuracy. \hfill ($10\%$)

```{r}
# With scaling.
modelSvmRadial <- svm(
  quality ~ .,
  data=dataTrain,
  kernel='radial',
  cost=100,
  gamma=1,
  scale=TRUE
)
pred <- predict(modelSvmRadial, newdata=dataTest[, -12])
mean(pred == dataTest[, 12])

# Without scaling.
modelSvmRadial <- svm(
  quality ~ .,
  data=dataTrain,
  kernel='radial',
  cost=100,
  gamma=1,
  scale=FALSE
)
pred <- predict(modelSvmRadial, newdata=dataTest[, -12])
mean(pred == dataTest[, 12])

```

The optimal value of the cost and gamma hyperparameters are set to 100 and 1 respectively. The classification accuracy is 64% when trained with scaling, and 48.75% when trained without scaling. This time, scaling resulted in better testing accuracy, which is what is expected.

e)  Train a logistic regression model. Then use the testing dataset to conduct an ROC curve analysis to compare the predictive performance of the trained logistic regression model and those two svm classifiers trained by using linear and RBF kernels respectively. \hfill ($10\%$)

```{r}
# Model the logistic regression.
modelLogisticReg <- glm(quality ~ ., data=dataTrain, family='binomial')
# Obtain predictions in the form of probabilities.
probabilities <- predict(modelLogisticReg, newdata=dataTest, type='response')
predictions <- prediction(probabilities, dataTest$quality)
tprfpr <- performance(predictions, measure='tpr', x.measure='fpr')
plot(tprfpr)
# Select the 'top left' corner of the plot, which is row 215 of tprfpr with
# threshold=0.4332222525, x=0.302816901, and y=0.763565891
classPredictions <- rep('Good', 400)
classPredictions[probabilities < 0.4332222525] <- 'Bad'
print(mean(classPredictions == dataTest$quality))

```

The classification accuracy of the logistic regression model is 73.75%. This is the highest, followed by the linear kernel SVM classifier (68.25%/69.75%), then the RBF kernel SVM classifier (64%). However, many factors can affect the evaluated performance. A more extreme threshold selected from the ROC curve may produce a better classification accuracy for this specific test dataset but may not generalise better to new test datasets. The three models were trained with all the possible predictors, but changing the selected predictors may result in better performances for each model. For the logistic regression model, dropping predictors whose p-values do not meet our desired thresholds may improve performance, as well as looking at pairwise correlations between the predictors.

```{r}
summary(modelLogisticReg)

# Remove predictors with p-values above 0.05.
modelLogisticReg <- glm(
  quality ~ .-citric.acid-chlorides-free.sulfur.dioxide-alcohol,
  data=dataTrain,
  family='binomial')
probabilities <- predict(modelLogisticReg, newdata=dataTest, type='response')
predictions <- prediction(probabilities, dataTest$quality)
tprfpr <- performance(predictions, measure='tpr', x.measure='fpr')
plot(tprfpr)
# Select the 'top left' corner of the plot, which is row 214 of tprfpr with
# threshold=0.4376569330, x=0.309859155, and y=0.763565891
classPredictions <- rep('Good', 400)
classPredictions[probabilities < 0.4376569330] <- 'Bad'
print(mean(classPredictions == dataTest$quality))

# Experiment with removing the same predictors as above.
set.seed(1)
tunedSvmLinear <- tune(
  svm,
  quality ~ .-citric.acid-chlorides-free.sulfur.dioxide-alcohol,
  data=dataTrain,
  kernel='linear',
  ranges=list(cost=c(0.01, 1, 100))
)
pred <- predict(tunedSvmLinear$best.model, newdata=dataTest[, -12])
mean(pred == dataTest[, 12])

set.seed(1)
tunedSvmRadial <- tune(
  svm,
  quality ~ .-citric.acid-chlorides-free.sulfur.dioxide-alcohol,
  data=dataTrain,
  kernel='radial',
  ranges=list(
    cost=c(0.01, 1, 100),
    gamma=c(0.01, 1, 100)
  )
)
pred <- predict(tunedSvmRadial$best.model, newdata=dataTest[, -12])
mean(pred == dataTest[, 12])

```

After removing some predictors for the logistic regression model, the classification accuracy was slightly improved, and another benefit is that the model is more explainable. The same result was observed for the linear and RBF kernel SVM classifiers with scaling, with a bigger performance improvement for the RBF kernel SVM classifier. 
